{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azorahai/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('labelledData.txt', sep=\",,,\", header = None)\n",
    "data.columns = [\"question\", \"type\"]\n",
    "#print(data)\n",
    "\n",
    "#shuffle data\n",
    "data = shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSome of features i can think of-\\n1. wh-word - check which wh word is present in a question.\\n2. each of wh-word is somewhat different, so, we need to create additional features to capture differences between \\nthem.\\n\\nLearning a model for a dataset is an iterative process, u create a model, test it, improve upon it, retest..so on.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the labelled data file has 5 classes in which a question belongs to, 'what', 'when', 'who',\n",
    "#'affirmative' 'unknown'.\n",
    "\n",
    "#We can use input sentence to construct a feature vector.\n",
    "#But, for that first we need to decide what features to have on data set.\n",
    "\n",
    "'''\n",
    "Some of features i can think of-\n",
    "1. wh-word - check which wh word is present in a question.\n",
    "2. each of wh-word is somewhat different, so, we need to create additional features to capture differences between \n",
    "them.\n",
    "\n",
    "Learning a model for a dataset is an iterative process, u create a model, test it, improve upon it, retest..so on.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some basic text processing functions we can use later!\n",
    "def tokenize(question):\n",
    "    tokens = nltk.word_tokenize(question)\n",
    "    return tokens\n",
    "    \n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed\n",
    "    \n",
    "s = \"You are looking lovely today!\"\n",
    "tokens = tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when': 96, 'who': 402, 'affirmation': 104, 'unknown': 272, 'what': 609}\n",
      "\n",
      "\n",
      "misclassified questions containing who\n",
      "type( unknown) question(who is the man behind the pig-the man who pulls the strings and speaks for miss piggy ? )\n",
      "type( what) question(what was the name of the lawyer who represented randy craft ? )\n",
      "type( what) question(who was the first host of person to person ? )\n",
      "type( unknown) question(name the ranger who was always after yogi bear . ? )\n",
      "type( what) question(who was the star of the 1965 broadway hit golden boy ? )\n",
      "type( what) question(who portrayed portly criminologist carl hyatt on checkmate ? )\n",
      "type( what) question(what soap was touted as being `` for people who like people '' ? )\n",
      "type( what) question(what is the full name of the man who invented the multicolored game cube that has 42.3 quintillion potential combinations ? )\n",
      "\n",
      "\n",
      "misclassified questions containing what\n",
      "type( unknown) question(what game 's board shows the territories of irkutsk , yakutsk and kamchatka ? )\n",
      "type( unknown) question(what is the name of the managing director of apricot computer ? )\n",
      "type( unknown) question(what is the occupation of nicholas cage ? )\n",
      "type( unknown) question(how can i find out what year a beanie baby was introduced ? )\n",
      "type( when) question(what time of day did emperor hirohito die ? )\n",
      "type( unknown) question(in what u.s. state was the first woman governor elected ? )\n",
      "type( when) question(what year did hitler die ? )\n",
      "type( when) question(when reading classified ads , what does eenty : other stand for ? )\n",
      "type( unknown) question(in what olympic games did nadia comaneci become popular ? )\n",
      "type( unknown) question(what does a nihilist believe in ? )\n",
      "type( unknown) question(what did richard feynman say upon hearing he would receive the nobel prize in physics ? )\n",
      "type( unknown) question(in what sport are these following numbers relevant : 118 , 126 , 134 , 142 , 15 , 158 , 167 , 177 , and 19 ? )\n",
      "type( unknown) question(what lawyer won the largest divorce settlement , $85 million , in u.s. history for sheika dena al-farri ? )\n",
      "type( when) question(what time does the flight leave ? )\n",
      "type( when) question(what time does the train arrive ? )\n",
      "type( when) question(what time do we go to the mall ? )\n",
      "type( when) question(what time are we leaving ? )\n",
      "type( when) question(what time does the president give his speech ? )\n",
      "type( when) question(what time do you go to the school ? )\n",
      "type( when) question(what time are you planning to leave ? )\n",
      "type( when) question(what time are you thinking of going to the dentist ? )\n",
      "type( when) question(what time is it right now ? )\n",
      "type( who) question(what actor came to dinner in guess who 's coming to dinner ? )\n",
      "type( who) question(who said : `` what contemptible scoundrel stole the cork from my lunch ? '' )\n",
      "type( when) question(when the tutankhamun exhibit was on display in the u.s. , what moving company transported it ? )\n",
      "type( when) question(when mighty mouse was conceived , what was his original name ? )\n",
      "type( when) question(when it 's time to relax , what one beer stands clear ? )\n",
      "type( when) question(what singer 's theme song was when the moon comes over the mountain ? )\n",
      "type( when) question(when not adventuring on rann , what does adam strange call his profession ? )\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let's begin with some basic feature-engineering.    \n",
    "'''\n",
    "labels = {'what' : 0,\n",
    "          'who' : 0,\n",
    "          'when' : 0,\n",
    "          'unknown' : 0,\n",
    "          'affirmation' : 0}\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    labels[type] = labels[type] + 1\n",
    "\n",
    "print(labels)\n",
    "#So, we see there is fair representation of each class type!!\n",
    "\n",
    "'''\n",
    "We need to train features that capture the DISTINCT trait of each type of question.\n",
    "To understand and use this structure, pos tags will be handy.\n",
    "'''\n",
    "\n",
    "#Let's look at affirmation or yes/no sentences to begin with.\n",
    "'''\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type == 'affirmation':\n",
    "        #print(data['question'][index])\n",
    "'''\n",
    "        \n",
    "#We can observe that unlike other question types, an affirmation always begins with a BE-verb or an\n",
    "#auxiliary verb.\n",
    "#This is a strong feature to identify affirmations.\n",
    "\n",
    "#WHO -> this question type asks about a person.\n",
    "\n",
    "#lets look at how many sentences which begin with who, get labelled as other classes.\n",
    "\n",
    "print(\"\\n\\nmisclassified questions containing who\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type != 'who' and 'who ' in data['question'][index].lower():\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))\n",
    "        \n",
    "#so, there are few such misclassified 'who' questions.\n",
    "\n",
    "#let's check for 'what' now.\n",
    "\n",
    "print(\"\\n\\nmisclassified questions containing what\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type != 'what' and 'what' in data['question'][index].lower():\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misclassified questions containing when\n",
      "type( what) question(when did rococo painting and architecture flourish ? )\n",
      "type( unknown) question(when did the bounty mutiny take place ? )\n",
      "type( unknown) question(when was the first wall street journal published ? )\n",
      "type( unknown) question(when did the berlin wall go up ? )\n",
      "type( what) question(when is boxing day ? )\n",
      "type( what) question(when is the tulip festival in michigan ? )\n",
      "type( what) question(when was hurricane hugo ? )\n",
      "type( what) question(when was `` the great depression '' ? )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLooking at the cases where when questions get labelled as what, this seems to happen\\nwhen there is no verb except BE-VERB suggesting any action happening.\\neg. \\nwhen was hurricane hugo ?\\nwhen was `` the great depression '' ? \\n\\nBUT, we can't act on this conclusion, since this doesn't always happen!\\n\\nFollowing two sentences don't have much difference in pos tags, but yet labelled differently!\\n\\n> when is boxing day - LABEL(what)\\n[('when', 'WRB'), ('is', 'VBZ'), ('boxing', 'VBG'), ('day', 'NN'), ('?', '.')]\\n\\n> when is bastille day - LABEL(when)\\n\\n[('when', 'WRB'), ('is', 'VBZ'), ('bastille', 'VBN'), ('day', 'NN'), ('?', '.')]\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So certainly there are misclassifications, let's construct a an initial model, and we will relook\n",
    "#at misclassifications/confusion \n",
    "#matrix to look at misclassifications.\n",
    "\n",
    "'''\n",
    "A question of type 'when' asks the time of occurence of certain event.\n",
    "Therefore, words such as 'time', and verbs that indicate some event or action, can act as strong features\n",
    "to detect a when type of question.\n",
    "'''\n",
    "#Label - 'when' ,\n",
    "\n",
    "print(\"misclassified questions containing when\")\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type != 'when' and 'when' == data['question'][index].split()[0].lower():#first word is when\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))\n",
    "\n",
    "'''\n",
    "Looking at the cases where when questions get labelled as what, this seems to happen\n",
    "when there is no verb except BE-VERB suggesting any action happening.\n",
    "eg. \n",
    "when was hurricane hugo ?\n",
    "when was `` the great depression '' ? \n",
    "\n",
    "BUT, we can't act on this conclusion, since this doesn't always happen!\n",
    "\n",
    "Following two sentences don't have much difference in pos tags, but yet labelled differently!\n",
    "\n",
    "> when is boxing day - LABEL(what)\n",
    "[('when', 'WRB'), ('is', 'VBZ'), ('boxing', 'VBG'), ('day', 'NN'), ('?', '.')]\n",
    "\n",
    "> when is bastille day - LABEL(when)\n",
    "\n",
    "[('when', 'WRB'), ('is', 'VBZ'), ('bastille', 'VBN'), ('day', 'NN'), ('?', '.')]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "misclassified questions containing what\n",
      "type( unknown) question(what game 's board shows the territories of irkutsk , yakutsk and kamchatka ? )\n",
      "type( unknown) question(what is the name of the managing director of apricot computer ? )\n",
      "type( unknown) question(what is the occupation of nicholas cage ? )\n",
      "type( when) question(what time of day did emperor hirohito die ? )\n",
      "type( when) question(what year did hitler die ? )\n",
      "type( unknown) question(what does a nihilist believe in ? )\n",
      "type( unknown) question(what did richard feynman say upon hearing he would receive the nobel prize in physics ? )\n",
      "type( unknown) question(what lawyer won the largest divorce settlement , $85 million , in u.s. history for sheika dena al-farri ? )\n",
      "type( when) question(what time does the flight leave ? )\n",
      "type( when) question(what time does the train arrive ? )\n",
      "type( when) question(what time do we go to the mall ? )\n",
      "type( when) question(what time are we leaving ? )\n",
      "type( when) question(what time does the president give his speech ? )\n",
      "type( when) question(what time do you go to the school ? )\n",
      "type( when) question(what time are you planning to leave ? )\n",
      "type( when) question(what time are you thinking of going to the dentist ? )\n",
      "type( when) question(what time is it right now ? )\n",
      "type( who) question(what actor came to dinner in guess who 's coming to dinner ? )\n",
      "type( when) question(what singer 's theme song was when the moon comes over the mountain ? )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLooking at the misclassifications, some things are immediately obvious,\\n\\n- using what to ask time, amounts to making a when question,\\n\\n- asking about a person is actually a who question(what actor came to dinner in guess who 's coming to dinner ?)\\n\\n- both 'time' and 'year' relate to time, so we need some mechanism to identify such synonyms or \\n    similar meaning words.\\n\\n- what question, that try to ask about a certain entity(PROPER NOUN), \\n  eg. nicholas cage, apricot computer, richard etc.which might itself be unknown, also get labelled as\\n  unknown!! Is this assumption/inference well found, let's assign features to find out better!!\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label - 'what'\n",
    "\n",
    "#What question usually asks for some information, clarify some fact.\n",
    "\n",
    "#Let's first look at questions that begin with 'what' but are labelled differently.\n",
    "print(\"\\n\\nmisclassified questions containing what\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type != 'what' and 'what' == data['question'][index].split()[0].lower():#first word is when\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))\n",
    "        \n",
    "'''\n",
    "Looking at the misclassifications, some things are immediately obvious,\n",
    "\n",
    "- using what to ask time, amounts to making a when question,\n",
    "\n",
    "- asking about a person is actually a who question(what actor came to dinner in guess who 's coming to dinner ?)\n",
    "\n",
    "- both 'time' and 'year' relate to time, so we need some mechanism to identify such synonyms or \n",
    "    similar meaning words.\n",
    "\n",
    "- what question, that try to ask about a certain entity(PROPER NOUN), \n",
    "  eg. nicholas cage, apricot computer, richard etc.which might itself be unknown, also get labelled as\n",
    "  unknown!! Is this assumption/inference well found, let's assign features to find out better!!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "misclassified questions containing who\n",
      "type( unknown) question(who is the man behind the pig-the man who pulls the strings and speaks for miss piggy ? )\n",
      "type( what) question(who was the first host of person to person ? )\n",
      "type( what) question(who was the star of the 1965 broadway hit golden boy ? )\n",
      "type( what) question(who portrayed portly criminologist carl hyatt on checkmate ? )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLooking at the misclassifications, \\n3 sentences are labelled as of \\'what\\' type, but it seems they should have been labelled \\'who\\' only.\\nSo, mostly, question that begin with \\'who\\' word, tend to be who questions only.\\nSo, a feature that we have to include is begin word of sentence.\\n\\nfor index in range(len(data)):\\n    type = data[\\'type\\'][index].strip()\\n    if type == \\'who\\':\\n        print(\"type({}) question({})\".format(data[\\'type\\'][index], data[\\'question\\'][index]))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label - 'who'\n",
    "\n",
    "#Who question usually asks about a person.\n",
    "\n",
    "#Let's first look at questions that begin with 'what' but are labelled differently.\n",
    "print(\"\\n\\nmisclassified questions containing who\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type != 'who' and 'who' == data['question'][index].split()[0].lower():#first word is when\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))\n",
    "'''\n",
    "Looking at the misclassifications, \n",
    "3 sentences are labelled as of 'what' type, but it seems they should have been labelled 'who' only.\n",
    "So, mostly, question that begin with 'who' word, tend to be who questions only.\n",
    "So, a feature that we have to include is begin word of sentence.\n",
    "\n",
    "for index in range(len(data)):\n",
    "    type = data['type'][index].strip()\n",
    "    if type == 'who':\n",
    "        print(\"type({}) question({})\".format(data['type'][index], data['question'][index]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Label - 'affirmation'\n",
    "\n",
    "'''\n",
    "An affirmaton is a yes/no question, and it mostly begins with a auxiliary-verb.\n",
    "Let's see if it has some less-obvious labelled examples.\n",
    "'''\n",
    "\n",
    "#So, to classify a question as affirmation, \n",
    "#it shouldn't be beginning with one of the wh-words, and\n",
    "#first word shall be a auxiliary verb.\n",
    "\n",
    "#Lets look at pos tags for some of affirmative sentences.\n",
    "def pos_tag(question):\n",
    "    tokens = tokenize(question)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Label - 'unknown'\n",
    "#A question, that doesn't associate well with any of the other 4 classes, will be classified\n",
    "#as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'WP'), ('time', 'NN'), ('is', 'VBZ'), ('it', 'PRP'), ('right', 'RB'), ('now', 'RB'), ('?', '.')]\n",
      "{'preceding_word_pos': 'RB', 'question_class': 'which', 'leading_question_word': 'which'}\n"
     ]
    }
   ],
   "source": [
    "#Model creation\n",
    "auxiliary_verbs = ['can', 'could', 'shall', 'should', 'do', 'does', 'did', 'am', 'is', 'are', 'was', 'were',\n",
    "                   'will', 'would', 'has', 'have', 'had']\n",
    "\n",
    "personal_pronouns = ['i', 'you', 'she', 'we', 'they', 'there', 'anybody', 'anyone', 'somebody', 'someone']\n",
    "\n",
    "time_synonyms = ['day', 'month', 'week', 'year', 'time']\n",
    "\n",
    "included_question_classes = ['who','when','what']\n",
    "excluded_question_classes = ['how', 'which']\n",
    "\n",
    "#this function defines a feature question_class - class that a question belongs to\n",
    "#possible values = {'who', 'what', 'when', 'affirmative', 'excluded'}\n",
    "def question_class(question, features):\n",
    "    if 'question_class' in features.keys() and 'question_class' == 'excluded_class':\n",
    "        return features\n",
    "    first_word = question.split()[0].strip()\n",
    "    dictionary = {'question_class':'unknown'}\n",
    "    \n",
    "    if first_word in auxiliary_verbs:\n",
    "        dictionary[\"question_class\"] = \"affirmative\"\n",
    "        dictionary['leading_question_word'] = 'affirmative'\n",
    "    else:\n",
    "        features['question_class'] = features['leading_question_word']\n",
    "        return features\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "#this function finds out, if sentence has a wh-word(what, who, when),\n",
    "#in case of multiple wh-words, it returns the first one found.\n",
    "def leading_question_word(question):\n",
    "    dictionary = {\"leading_question_word\":'unknown'}\n",
    "    for word in question.split():\n",
    "        if word in included_question_classes:\n",
    "            dictionary[\"leading_question_word\"] = word\n",
    "            break\n",
    "        else:\n",
    "            if word in excluded_question_classes:\n",
    "                dictionary['leading_question_word'] = 'unknown'\n",
    "                break\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "#this method finds and returns first word of question as a feature.\n",
    "def first_word(question):\n",
    "    dictionary = {}\n",
    "    dictionary[\"first_word\"] = question.split()[0]\n",
    "    return dictionary\n",
    "\n",
    "#this function finds and returns the first word in question that follows a leading_question_word.\n",
    "def following_word_feature(question_tokens, features):\n",
    "    dictionary = {}\n",
    "    if features[\"leading_question_word\"] != 'unknown':\n",
    "        index = question_tokens.index(features[\"leading_question_word\"])\n",
    "        if index < len(question_tokens) - 1:\n",
    "            next_word = question_tokens[index+1].strip()\n",
    "            if next_word in time_synonyms:\n",
    "                features[\"leading_question_word\"] = 'which'\n",
    "                features[\"question_class\"] = \"excluded_class\"\n",
    "    return features\n",
    "\n",
    "#this function finds and returns the preceding word if any, to the leading_question_word.\n",
    "def preceding_word_feature(question_tokens, features):\n",
    "    question_tagged = nltk.pos_tag(question_tokens)\n",
    "    dictionary = {}\n",
    "    if features['leading_question_word'] == 'what':\n",
    "        index = question_tokens.index('what')\n",
    "        dictionary['preceding_word_pos'] = question_tagged[index-1][1]\n",
    "    return dictionary\n",
    "\n",
    "def question_features(question):\n",
    "    question = question.strip(\"[ .?]\")\n",
    "    question_tokens = tokenize(question)\n",
    "    features = {}\n",
    "    features = {**leading_question_word(question), **features}\n",
    "    features = {**preceding_word_feature(question_tokens, features), **features}\n",
    "    features = following_word_feature(question_tokens, features)\n",
    "    features = question_class(question, features)\n",
    "    return features\n",
    "\n",
    "s = \"what time is it right now ?\"\n",
    "print(nltk.pos_tag((s).split()))\n",
    "print(question_features(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Divide into training and test datasets\n",
    "feature_sets = [(question_features(row['question']), row['type'].strip()) for (index, row) in data.iterrows()]\n",
    "\n",
    "#Since the dataset is very small(1483 samples), we will divide in 80:20 ratio in training and test sets. \n",
    "train_set, test_set = feature_sets[:1186], feature_sets[1186:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   leading_question_word = 'what'           what : who    =    206.3 : 1.0\n",
      "          question_class = 'what'           what : who    =    206.3 : 1.0\n",
      "          question_class = 'who'             who : what   =    192.7 : 1.0\n",
      "   leading_question_word = 'who'             who : what   =    192.7 : 1.0\n",
      "   leading_question_word = 'when'           when : what   =     75.4 : 1.0\n",
      "          question_class = 'when'           when : what   =     75.4 : 1.0\n",
      "      preceding_word_pos = None              who : what   =     63.9 : 1.0\n",
      "   leading_question_word = 'unknown'      unknow : affirm =     52.4 : 1.0\n",
      "          question_class = 'unknown'      unknow : affirm =     52.4 : 1.0\n",
      "      preceding_word_pos = 'NN'             what : unknow =     35.0 : 1.0\n",
      "Accuracy on training set:(0.978920741989882)\n",
      "Accuracy on test set:(0.9797979797979798)\n",
      "accuracy:(0.9745762711864406)\n",
      "accuracy:(0.9745762711864406)\n",
      "accuracy:(0.9661016949152542)\n",
      "accuracy:(0.9915254237288136)\n",
      "accuracy:(0.9661016949152542)\n",
      "accuracy:(0.9915254237288136)\n",
      "accuracy:(0.9914529914529915)\n",
      "accuracy:(0.9743589743589743)\n",
      "accuracy:(0.9829059829059829)\n",
      "accuracy:(0.9829059829059829)\n",
      "Accuracy on training set:(0.9772344013490725)\n",
      "Accuracy on test set:(0.9797979797979798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azorahai/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Using a naive bayes classifier.\n",
    "classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#Most informative features.\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "#accuracy\n",
    "print(\"Accuracy on training set:({})\".format(nltk.classify.accuracy(classifier, train_set)))\n",
    "\n",
    "print(\"Accuracy on test set:({})\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "\n",
    "'''\n",
    "With our initial model, we got a 96.6% accuracy on test_set, \n",
    "After repeated iteration, we have got a accuracy of 98% on training set, \n",
    "but an accuracy of 99% on test set.\n",
    "\n",
    "To avoid over-fitting, let us perform 10-fold cross-validation on training dataset.\n",
    "'''\n",
    "from sklearn import cross_validation\n",
    "cv = cross_validation.KFold(len(train_set), n_folds=10, shuffle=False, random_state=None)\n",
    "\n",
    "for traincv, evalcv in cv:\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    print('accuracy:({})'.format(nltk.classify.accuracy(classifier, train_set[evalcv[0]:evalcv[len(evalcv)-1]])))\n",
    "    \n",
    "#after cross-validation on training set, prediction accuracy\n",
    "print(\"Accuracy on training set:({})\".format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print(\"Accuracy on test set:({})\".format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:(0.9797639123102867)\n",
      "Accuracy on test set:(0.9797979797979798)\n",
      "what\n",
      "accuracy:(0.9745762711864406)\n",
      "accuracy:(0.9745762711864406)\n",
      "accuracy:(0.9661016949152542)\n",
      "accuracy:(0.9915254237288136)\n",
      "accuracy:(0.9661016949152542)\n",
      "accuracy:(1.0)\n",
      "accuracy:(0.9914529914529915)\n",
      "accuracy:(0.9743589743589743)\n",
      "accuracy:(0.9829059829059829)\n",
      "accuracy:(0.9829059829059829)\n",
      "After 10-fold cross-validation\n",
      "Accuracy on training set:(0.9797639123102867)\n",
      "Accuracy on test set:(0.9797979797979798)\n"
     ]
    }
   ],
   "source": [
    "#Using a Decision tree classifier.\n",
    "classifier = nltk.classify.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "#Most informative features.\n",
    "#classifier.show_most_informative_features()\n",
    "\n",
    "#accuracy\n",
    "print(\"Accuracy on training set:({})\".format(nltk.classify.accuracy(classifier, train_set)))\n",
    "\n",
    "print(\"Accuracy on test set:({})\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "\n",
    "print(classifier.classify(question_features('hazmat stands for what?')))\n",
    "\n",
    "'''\n",
    "With our initial model, we got a 96.6% accuracy on test_set, \n",
    "After repeated iteration, we have got a accuracy of 98% on training set, \n",
    "but an accuracy of 99% on test set.\n",
    "\n",
    "To avoid over-fitting, let us perform 10-fold cross-validation on training dataset.\n",
    "'''\n",
    "from sklearn import cross_validation\n",
    "\n",
    "cv = cross_validation.KFold(len(train_set), n_folds=10, shuffle=False, random_state=None)\n",
    "\n",
    "for traincv, evalcv in cv:\n",
    "    classifier = nltk.classify.DecisionTreeClassifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    print('accuracy:({})'.format(nltk.classify.accuracy(classifier, train_set[evalcv[0]:evalcv[len(evalcv)-1]])))\n",
    "    \n",
    "#after cross-validation on training set, prediction accuracy\n",
    "print(\"After 10-fold cross-validation\")\n",
    "print(\"Accuracy on training set:({})\".format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print(\"Accuracy on test set:({})\".format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.476 preceding_word_pos=='VB' and label is 'who'\n",
      "  -3.132 question_class=='what' and label is 'who'\n",
      "  -3.132 leading_question_word=='what' and label is 'who'\n",
      "   2.882 preceding_word_pos=='IN' and label is 'unknown'\n",
      "   2.569 question_class=='affirmative' and label is 'affirmation'\n",
      "   2.569 leading_question_word=='affirmative' and label is 'affirmation'\n",
      "   2.444 question_class=='which' and label is 'when'\n",
      "   2.444 leading_question_word=='which' and label is 'when'\n",
      "   2.379 question_class=='unknown' and label is 'unknown'\n",
      "   2.379 leading_question_word=='unknown' and label is 'unknown'\n",
      "0.9797639123102867\n",
      "0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "#Using a Maxent classifier.\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, 'GIS', trace=0, max_iter=100)\n",
    "\n",
    "#Most informative features.\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "#accuracy\n",
    "print(nltk.classify.accuracy(classifier, train_set))\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of errors are:(30)\n",
      "actual=affirmation predicted=unknown  question=it's written that the dimensions are 7x7x4inches, is it right?                                      \n",
      "actual=affirmation predicted=unknown  question=which water filter cap replacement (white color) should i use for this filter ?                     \n",
      "actual=unknown  predicted=what     question=in what olympic games did nadia comaneci become popular ?                                           \n",
      "actual=unknown  predicted=what     question=in what sport are these following numbers relevant : 118 , 126 , 134 , 142 , 15 , 158 , 167 , 177 , and 19 ?\n",
      "actual=unknown  predicted=what     question=in what u.s. state was the first woman governor elected ?                                           \n",
      "actual=unknown  predicted=what     question=what did richard feynman say upon hearing he would receive the nobel prize in physics ?             \n",
      "actual=unknown  predicted=what     question=what does a nihilist believe in ?                                                                   \n",
      "actual=unknown  predicted=what     question=what game 's board shows the territories of irkutsk , yakutsk and kamchatka ?                       \n",
      "actual=unknown  predicted=what     question=what is the name of the managing director of apricot computer ?                                     \n",
      "actual=unknown  predicted=what     question=what is the occupation of nicholas cage ?                                                           \n",
      "actual=unknown  predicted=what     question=what lawyer won the largest divorce settlement , $85 million , in u.s. history for sheika dena al-farri ?\n",
      "actual=unknown  predicted=when     question=when did the berlin wall go up ?                                                                    \n",
      "actual=unknown  predicted=when     question=when did the bounty mutiny take place ?                                                             \n",
      "actual=unknown  predicted=when     question=when was the first wall street journal published ?                                                  \n",
      "actual=unknown  predicted=who      question=name the ranger who was always after yogi bear . ?                                                  \n",
      "actual=unknown  predicted=who      question=who is the man behind the pig-the man who pulls the strings and speaks for miss piggy ?             \n",
      "actual=what     predicted=when     question=what year did germany sign its nonaggression pact with the soviet union ?                           \n",
      "actual=what     predicted=when     question=what year did jack nicklaus join the professional golfers association tour ?                        \n",
      "actual=what     predicted=when     question=what year did the united states pass the copyright law ?                                            \n",
      "actual=what     predicted=when     question=what year did the vietnam war end ?                                                                 \n",
      "actual=what     predicted=when     question=when did rococo painting and architecture flourish ?                                                \n",
      "actual=what     predicted=when     question=when is boxing day ?                                                                                \n",
      "actual=what     predicted=when     question=when is the tulip festival in michigan ?                                                            \n",
      "actual=what     predicted=when     question=when was `` the great depression '' ?                                                               \n",
      "actual=what     predicted=when     question=when was hurricane hugo ?                                                                           \n",
      "actual=what     predicted=who      question=who portrayed portly criminologist carl hyatt on checkmate ?                                        \n",
      "actual=what     predicted=who      question=who was the first host of person to person ?                                                        \n",
      "actual=what     predicted=who      question=who was the star of the 1965 broadway hit golden boy ?                                              \n",
      "actual=when     predicted=what     question=what singer 's theme song was when the moon comes over the mountain ?                               \n",
      "actual=who      predicted=what     question=what actor came to dinner in guess who 's coming to dinner ?                                        \n"
     ]
    }
   ],
   "source": [
    "#Let's have a quick look at the errors made in classification\n",
    "errors = []\n",
    "for i, row in data.iterrows():\n",
    "    type = row['type'].strip()\n",
    "    question = row['question'].strip()\n",
    "    guess = classifier.classify(question_features(question))\n",
    "    if guess != type:\n",
    "        errors.append( (type, guess, question) )\n",
    "\n",
    "print('No of errors are:({})'.format(len(errors)))\n",
    "for (type, guess, question) in sorted(errors):\n",
    "    print('actual={:<8} predicted={:<8s} question={:<100}'.format(type, guess, question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThere are a total of 31 misclassified examples from the complete dataset,\\n\\nsome one these appear to be wrongly labelled and hence will also be misclassified by the learning algo,\\nlisting 25 such below, there are probably even more:\\n\\nactual=unknown?  predicted=what     question=what did richard feynman say upon hearing he would receive the nobel prize in physics ?             \\nactual=unknown?  predicted=what     question=what does a nihilist believe in ?                                                                   \\nactual=unknown?  predicted=what     question=what game 's board shows the territories of irkutsk , yakutsk and kamchatka ?                       \\nactual=unknown?  predicted=what     question=what is the name of the managing director of apricot computer ?                                     \\nactual=unknown?  predicted=what     question=what is the occupation of nicholas cage ?                                                           \\nactual=unknown?  predicted=what     question=what lawyer won the largest divorce settlement , $85 million , in u.s. history for sheika dena al-farri ?\\nactual=unknown?  predicted=when     question=when did the berlin wall go up ?                                                                    \\nactual=unknown?  predicted=when     question=when did the bounty mutiny take place ?                                                             \\nactual=unknown?  predicted=when     question=when was the first wall street journal published ?                                                  \\nactual=unknown?  predicted=who      question=name the ranger who was always after yogi bear . ?                                                  \\nactual=unknown?  predicted=who      question=who is the man behind the pig-the man who pulls the strings and speaks for miss piggy ?             \\nactual=what??     predicted=when     question=what year did germany sign its nonaggression pact with the soviet union ?                           \\nactual=what??     predicted=when     question=what year did jack nicklaus join the professional golfers association tour ?                        \\nactual=what??     predicted=when     question=what year did the united states pass the copyright law ?                                            \\nactual=what??     predicted=when     question=what year did the vietnam war end ?                                                                 \\nactual=what??     predicted=when     question=when did rococo painting and architecture flourish ?                                                \\nactual=what??     predicted=when     question=when is boxing day ?                                                                                \\nactual=what??     predicted=when     question=when is the tulip festival in michigan ?                                                            \\nactual=what??     predicted=when     question=when was `` the great depression '' ?                                                               \\nactual=what??     predicted=when     question=when was hurricane hugo ?                                                                           \\nactual=what??     predicted=who      question=who portrayed portly criminologist carl hyatt on checkmate ?                                        \\nactual=what??     predicted=who      question=who was the first host of person to person ?                                                        \\nactual=what??     predicted=who      question=who was the star of the 1965 broadway hit golden boy ? \\n\\n\\nCONCLUSION: All 3 models Naive Bayes, DecisionTree, Maxent perform similar on test set,\\nand Naiye Bayes gives best accuracy of 98.9%\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conclusion\n",
    "'''\n",
    "There are a total of 30 misclassified examples from the complete dataset,\n",
    "\n",
    "Some of these appear to be wrongly labelled and hence will also be misclassified by the learning algo,\n",
    "listing 25 such below:\n",
    "\n",
    "actual=unknown?  predicted=what     question=what did richard feynman say upon hearing he would receive the nobel prize in physics ?             \n",
    "actual=unknown?  predicted=what     question=what does a nihilist believe in ?                                                                   \n",
    "actual=unknown?  predicted=what     question=what game 's board shows the territories of irkutsk , yakutsk and kamchatka ?                       \n",
    "actual=unknown?  predicted=what     question=what is the name of the managing director of apricot computer ?                                     \n",
    "actual=unknown?  predicted=what     question=what is the occupation of nicholas cage ?                                                           \n",
    "actual=unknown?  predicted=what     question=what lawyer won the largest divorce settlement , $85 million , in u.s. history for sheika dena al-farri ?\n",
    "actual=unknown?  predicted=when     question=when did the berlin wall go up ?                                                                    \n",
    "actual=unknown?  predicted=when     question=when did the bounty mutiny take place ?                                                             \n",
    "actual=unknown?  predicted=when     question=when was the first wall street journal published ?                                                  \n",
    "actual=unknown?  predicted=who      question=name the ranger who was always after yogi bear . ?                                                  \n",
    "actual=unknown?  predicted=who      question=who is the man behind the pig-the man who pulls the strings and speaks for miss piggy ?             \n",
    "actual=what??     predicted=when     question=what year did germany sign its nonaggression pact with the soviet union ?                           \n",
    "actual=what??     predicted=when     question=what year did jack nicklaus join the professional golfers association tour ?                        \n",
    "actual=what??     predicted=when     question=what year did the united states pass the copyright law ?                                            \n",
    "actual=what??     predicted=when     question=what year did the vietnam war end ?                                                                 \n",
    "actual=what??     predicted=when     question=when did rococo painting and architecture flourish ?                                                \n",
    "actual=what??     predicted=when     question=when is boxing day ?                                                                                \n",
    "actual=what??     predicted=when     question=when is the tulip festival in michigan ?                                                            \n",
    "actual=what??     predicted=when     question=when was `` the great depression '' ?                                                               \n",
    "actual=what??     predicted=when     question=when was hurricane hugo ?                                                                           \n",
    "actual=what??     predicted=who      question=who portrayed portly criminologist carl hyatt on checkmate ?                                        \n",
    "actual=what??     predicted=who      question=who was the first host of person to person ?                                                        \n",
    "actual=what??     predicted=who      question=who was the star of the 1965 broadway hit golden boy ? \n",
    "\n",
    "\n",
    "CONCLUSION: All 3 models Naive Bayes, DecisionTree, Maxent perform similar on test set,\n",
    "and Naiye Bayes gives best accuracy of 98%\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
